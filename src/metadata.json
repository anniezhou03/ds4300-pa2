["B-Trees The idea we saw earlier of putting multiple set (list, hash table) elements together into large chunks that exploit locality can also be applied to trees. Binary search trees are not good for locality because a given node of the binary tree probably occupies only a fraction of any cache line. B-trees are a way to get better locality by putting multiple elements into each tree node. B-trees were originally invented for storing data structures on disk, where locality is even more crucial than with memory. Accessing a disk location takes about 5ms = 5,000,000ns. Therefore, if you are storing a tree on disk, you want to make sure that a given disk read is as effective as possible. B-trees have a high branching factor, much larger than 2, which ensures that few disk reads are needed to navigate to the place where data is stored. B- trees may also useful for in-memory data structures because these days main memory is almost as slow relative to the processor as disk drives were to main memory when B-trees were first introduced! A B-tree of order m is a search tree in which each nonleaf node has up to m children. The actual elements of the collection are stored in the leaves of the tree, and the nonleaf nodes contain only keys. Each leaf stores some number of elements; the maximum number may be greater or (typically) less than m. The data structure satisfies several invariants: 1. Every path from the root to a leaf has the same length 2. If a node has n children, it contains n\u22121 keys. 3. Every node (except the root) is at least half full 4. The elements stored in a given subtree all have keys that are between the keys in the parent node", "root to a leaf has the same length 2. If a node has n children, it contains n\u22121 keys. 3. Every node (except the root) is at least half full 4. The elements stored in a given subtree all have keys that are between the keys in the parent node on either side of the subtree pointer. (This generalizes the BST invariant.) 5. The root has at least two children if it is not a leaf. For example, the following is an order-5 B-tree (m=5) where the leaves have enough space to store up to 3 data records: 3/18/25, 5:06 PM B-trees https://www.cs.cornell.edu/courses/cs3110/2012sp/recitations/rec25-B-trees/rec25.html 1/3", "Because the height of the tree is uniformly the same and every node is at least half full, we are guaranteed that the asymptotic performance is O(lg n) where n is the size of the collection. The real win is in the constant factors, of course. We can choose m so that the pointers to the m children plus the m\u22121 elements fill out a cache line at the highest level of the memory hierarchy where we can expect to get cache hits. For example, if we are accessing a large disk database then our \"cache lines\" are memory blocks of the size that is read from disk. Lookup in a B-tree is straightforward. Given a node to start from, we use a simple linear or binary search to find whether the desired element is in the node, or if not, which child pointer to follow from the current node. Insertion and deletion from a B-tree are more complicated; in fact, they are notoriously difficult to implement correctly. For insertion, we first find the appropriate leaf node into which the inserted element falls (assuming it is not already in the tree). If there is already room in the node, the new element can be inserted simply. Otherwise the current leaf is already full and must be split into two leaves, one of which acquires the new element. The parent is then updated to contain a new key and child pointer. If the parent is already full, the process ripples upwards, eventually possibly reaching the root. If the root is split into two, then a new root is created with just two children, increasing the height of the tree by one. For example, here is the effect of a series of insertions. The first insertion (13) merely affects a leaf. The", "eventually possibly reaching the root. If the root is split into two, then a new root is created with just two children, increasing the height of the tree by one. For example, here is the effect of a series of insertions. The first insertion (13) merely affects a leaf. The second insertion (14) overflows the leaf and adds a key to an internal node. The third insertion propagates all the way to the root. 3/18/25, 5:06 PM B-trees https://www.cs.cornell.edu/courses/cs3110/2012sp/recitations/rec25-B-trees/rec25.html 2/3", "Deletion works in the opposite way: the element is removed from the leaf. If the leaf becomes empty, a key is removed from the parent node. If that breaks invariant 3, the keys of the parent node and its immediate right (or left) sibling are reapportioned among them so that invariant 3 is satisfied. If this is not possible, the parent node can be combined with that sibling, removing a key another level up in the tree and possible causing a ripple all the way to the root. If the root has just two children, and they are combined, then the root is deleted and the new combined node becomes the root of the tree, reducing the height of the tree by one. Further reading: Aho, Hopcroft, and Ullman, Data Structures and Algorithms, Chapter 11. 3/18/25, 5:06 PM B-trees https://www.cs.cornell.edu/courses/cs3110/2012sp/recitations/rec25-B-trees/rec25.html 3/3", "Paris is the capital of france", "1/6/25: Introduction \u25cf\u200b need to install: \u25cb\u200b docker desktop \u25cb\u200b anaconda or miniconda python \u25cb\u200b datagrip or dbeaver \u25cb\u200b VS code for python development (3.10 or higher) \u25cb\u200b ability to interact with git and github \u25cf\u200b should review \u25cb\u200b shell cmds \u25cb\u200b docker and docker compose \u25a0\u200b whats a container, whats an image, volume, mapping \u25cf\u200b make an aws account 1/8/25: Foundations \u25cf\u200b baseline efficiency is linear search \u25cb\u200b start at beginning of list and progress from the first element to the last element until you find what youre looking for or get to the last element \u25cf\u200b record: a collection of values \u25cb\u200b ex. a row in a table \u25cf\u200b collection: set of records of the same entity type \u25cb\u200b ex. a table \u25cf\u200b search key: a value for an attribute from the entity type \u25cb\u200b ex. >1 \u25cf\u200b need n * x bytes of memory \u25cb\u200b each record takes up x bytes \u25cb\u200b n is the number of records \u25cf\u200b contiguously allocated list (array) \u25cb\u200b all n*x bytes are allocated as a single chunk of memory \u25cf\u200b linked list \u25cb\u200b each record needs x bytes + additional space for 1 or 2 memory addresses \u25cb\u200b takes up a bit more space \u25cb\u200b need some way to know if you are at the front of the list \u25cf\u200b arrays vs linked list pros and cons \u25cb\u200b arrays are faster for random access but slow for inserting anywhere but the end \u25cb\u200b linked lists are faster for inserting anywhere in the list, but slower for random access \u25cf\u200b binary search: cut in half each time and figure out which half target is in \u25cb\u200b has to be sorted \u25cb\u200b input: array of values in sorted order, target value \u25cb\u200b output: the location (index) of where target is located or some", "the list, but slower for random access \u25cf\u200b binary search: cut in half each time and figure out which half target is in \u25cb\u200b has to be sorted \u25cb\u200b input: array of values in sorted order, target value \u25cb\u200b output: the location (index) of where target is located or some value indicating target was not found \u25cb\u200b usually recursive", "\u25a0\u200b can be dangerous if you have a huge dataset \u25cf\u200b time complexity \u25cb\u200b linear search \u25a0\u200b best case: O(1) \u25a0\u200b worst case: O(n) \u25cb\u200b binary search for an array (contiguously allocated list) \u25a0\u200b best case: O(1) \u25a0\u200b worst case: O(log2n) \u25a0\u200b super inefficient on a linked list \u25cf\u200b database searching \u25cb\u200b problem: can only efficiently search a table buy the primary key (since it is the only sorted column) \u25cb\u200b we need an external data structure to support faster searching than a linear scan \u25cb\u200b solution: database index \u25cf\u200b binary search tree \u25cb\u200b has fast insert like a linked list and fast search like an array \u25cb\u200b every node in the left subtree is less than its parent and every node in the right subtree is greater than its parent 1/9/25 \u25cf\u200b 1/16/25 \u25cf\u200b need to recursively descend to find all the files?? \u25cf\u200b OOP in python \u25cb\u200b constructor \u25cb\u200b add_value function appends value to a list \u25cb\u200b consider duplicates using a set object \u25cb\u200b abstract method: if a class implements this method, then implementations of the class also need the method \u25cb\u200b pickle it def _rotate_left(self, x: AVLNode) -> AVLNode: \u200b T2 = y.left \u200b \u200b y.left = x \u200b x.right = T2 \u200b \u200b ht_x_left = (0 if not x.left else x.left.height) \u200b ht_x_right = (0 if not x.right else x.right.height) \u200b x.height = 1 + (ht_x_left if ( ht_x_left > \u200b", "1/27/25: Moving Beyond the Relational Model \u25cf\u200b benefits of the relational model \u25cb\u200b mostly standard data model and query language \u25cb\u200b ACID compliance \u25cb\u200b works well with highly structured and large amounts of data \u25cf\u200b ACID properties \u25cb\u200b atomicity \u25cb\u200b consistency \u25cb\u200b isolation \u25cb\u200b durability 2/19/25: Intro to the Graph Data Model \u25cf\u200b graph database \u25cb\u200b data model based on the graph data structure \u25cb\u200b composed of nodes and edges \u25cf\u200b types of graphs \u25cb\u200b connected (vs unconnected) \u25a0\u200b for every pair of nodes, there is a path \u25cb\u200b weighted (vs unweighted) \u25a0\u200b edge has a weight property \u25cb\u200b directed (vs undirected) \u25a0\u200b edges define a start and end node \u25cb\u200b acyclic (vs cyclic) \u25a0\u200b graph contains no cycles practical \u25cf\u200b pip install pymupdf \u25cf\u200b ollama pull mistral:latest", "ICS 46 Spring 2022 | News | Course Reference | Schedule | Project Guide | Notes and Examples | Reinforcement Exercises | Grade Calculator | About Alex ICS 46 Spring 2022 Notes and Examples: AVL Trees Why we must care about binary search tree balancing We've seen previously that the performance characteristics of binary search trees can vary rather wildly, and that they're mainly dependent on the shape of the tree, with the height of the tree being the key determining factor. By definition, binary search trees restrict what keys are allowed to present in which nodes \u2014 smaller keys have to be in left subtrees and larger keys in right subtrees \u2014 but they specify no restriction on the tree's shape, meaning that both of these are perfectly legal binary search trees containing the keys 1, 2, 3, 4, 5, 6, and 7. Yet, while both of these are legal, one is better than the other, because the height of the first tree (called a perfect binary tree) is smaller than the height of the second (called a degenerate tree). These two shapes represent the two extremes \u2014 the best and worst possible shapes for a binary search tree containing seven keys. Of course, when all you have is a very small number of keys like this, any shape will do. But as the number of keys grows, the distinction between these two tree shapes becomes increasingly vital. What's more, the degenerate shape isn't even necessarily a rare edge case: It's what you get when you start with an empty tree and add keys that are already in order, which is a surprisingly common scenario in real-world programs. For example, one very obvious algorithm for generating unique integer keys \u2014 when all you care about is that they're unique", "case: It's what you get when you start with an empty tree and add keys that are already in order, which is a surprisingly common scenario in real-world programs. For example, one very obvious algorithm for generating unique integer keys \u2014 when all you care about is that they're unique \u2014 is to generate them sequentially. 3/18/25, 5:04 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ 1/12", "What's so bad about a degenerate tree, anyway? Just looking at a picture of a degenerate tree, your intuition should already be telling you that something is amiss. In particular, if you tilt your head 45 degrees to the right, they look just like linked lists; that perception is no accident, as they behave like them, too (except that they're more complicated, to boot!). From a more analytical perspective, there are three results that should give us pause: Every time you perform a lookup in a degenerate binary search tree, it will take O(n) time, because it's possible that you'll have to reach every node in the tree before you're done. As n grows, this is a heavy burden to bear. If you implement your lookup recursively, you might also be using O(n) memory, too, as you might end up with as many as n frames on your run-time stack \u2014 one for every recursive call. There are ways to mitigate this \u2014 for example, some kinds of carefully-written recursion (in some programming languages, including C++) can avoid run-time stack growth as you recurse \u2014 but it's still a sign of potential trouble. The time it will take you to build the degenerate tree will also be prohibitive. If you start with an empty binary search tree and add keys to it in order, how long does it take to do it? The first key you add will go directly to the root. You could think of this as taking a single step: creating the node. The second key you add will require you to look at the root node, then take one step to the right. You could think of this as taking two steps. Each subsequent key you add will require one more step than the one before", "a single step: creating the node. The second key you add will require you to look at the root node, then take one step to the right. You could think of this as taking two steps. Each subsequent key you add will require one more step than the one before it. The total number of steps it would take to add n keys would be determined by the sum 1 + 2 + 3 + ... + n. This sum, which we'll see several times throughout this course, is equal to n(n + 1) / 2. So, the total number of steps to build the entire tree would be \u0398(n2). Overall, when n gets large, the tree would be hideously expensive to build, and then every subsequent search would be painful, as well. So this, in general, is a situation we need to be sure to avoid, or else we should probably consider a data structure other than a binary search tree; the worst case is simply too much of a burden to bear if n might get large. But if we can find a way to control the tree's shape more carefully, to force it to remain more balanced, we'll be fine. The question, of course, is how to do it, and, as importantly, whether we can do it while keeping the cost low enough that it doesn't outweigh the benefit. Aiming for perfection The best goal for us to shoot for would be to maintain perfection. In other words, every time we insert a key into our binary search tree, it would ideally still be a perfect binary tree, in which case we'd know that the height of the tree would always be \u0398(log n), with a commensurate effect on performance. However, when we consider this goal, a", "words, every time we insert a key into our binary search tree, it would ideally still be a perfect binary tree, in which case we'd know that the height of the tree would always be \u0398(log n), with a commensurate effect on performance. However, when we consider this goal, a problem emerges almost immediately. The following are all perfect binary trees, by definition: 3/18/25, 5:04 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ 2/12", "The perfect binary trees pictured above have 1, 3, 7, and 15 nodes respectively, and are the only possible perfect shapes for binary trees with that number of nodes. The problem, though, lies in the fact that there is no valid perfect binary tree with 2 nodes, or with 4, 5, 6, 8, 9, 10, 11, 12, 13, or 14 nodes. So, generally, it's impossible for us to guarantee that a binary search tree will always be \"perfect,\" by our definition, because there's simply no way to represent most numbers of keys. So, first thing's first: We'll need to relax our definition of \"perfection\" to accommodate every possible number of keys we might want to store. Complete binary trees A somewhat more relaxed notion of \"perfection\" is something called a complete binary tree, which is defined as follows. A complete binary tree of height h is a binary tree where: If h = 0, its left and right subtrees are empty. If h > 0, one of two things is true: The left subtree is a perfect binary tree of height h \u2212 1 and the right subtree is a complete binary tree of height h \u2212 1 The left subtree is a complete binary tree of height h \u2212 1 and the right subtree is a perfect binary tree of height h \u2212 2 That can be a bit of a mind-bending definition, but it actually leads to a conceptually simple result: On every level of a complete binary tree, every node that could possibly be present will be, except the last level might be missing nodes, but if it is missing nodes, the nodes that are there will be as far to the left as possible. The following are all complete binary trees: Furthermore, these are the only", "tree, every node that could possibly be present will be, except the last level might be missing nodes, but if it is missing nodes, the nodes that are there will be as far to the left as possible. The following are all complete binary trees: Furthermore, these are the only possible complete binary trees with these numbers of nodes in them; any other arrangement of, say, 6 keys besides the one shown above would violate the definition. We've seen that the height of a perfect binary tree is \u0398(log n). It's not a stretch to see that the height a complete binary tree will be \u0398(log n), as well, and we'll accept that via our intuition for now and proceed. All in all, a complete binary tree would be a great goal for us to attain: If we could keep the shape of our binary search trees complete, we would always have binary search trees with height \u0398(log n). The cost of maintaining completeness The trouble, of course, is that we need an algorithm for maintaining completeness. And before we go to the trouble of trying to figure one out, we should consider whether it's even worth our time. What can we deduce about the cost of maintaining completeness, even if we haven't figured out an algorithm yet? One example demonstrates a very big problem. Suppose we had the binary search tree on the left \u2014 which is complete, by our definition \u2014 and we wanted to insert the key 1 into it. If so, we would need an algorithm that would transform the tree on the left into the tree on the right. 3/18/25, 5:04 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ 3/12", "1 into it. If so, we would need an algorithm that would transform the tree on the left into the tree on the right. 3/18/25, 5:04 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ 3/12", "The tree on the right is certainly complete, so this would be the outcome we'd want. But consider what it would take to do it. Every key in the tree had to move! So, no matter what algorithm we used, we would still have to move every key. If there are n keys in the tree, that would take \u03a9(n) time \u2014 moving n keys takes at least linear time, even if you have the best possible algorithm for moving them; the work still has to get done. So, in the worst case, maintaining completeness after a single insertion requires \u03a9(n) time. Unfortunately, this is more time than we ought to be spending on maintaining balance. This means we'll need to come up with a compromise; as is often the case when we learn or design algorithms, our willingness to tolerate an imperfect result that's still \"good enough\" for our uses will often lead to an algorithm that is much faster than one that achieves a perfect result. So what would a \"good enough\" result be? What is a \"good\" balance condition Our overall goal is for lookups, insertions, and removals from a binary search tree to require O(log n) time in every case, rather than letting them degrade to a worst-case behavior of O(n). To do that, we need to decide on a balance condition, which is to say that we need to understand what shape is considered well- enough balanced for our purposes, even if not perfect. A \"good\" balance condition has two properties: The height of a binary search tree meeting the condition is \u0398(log n). It takes O(log n) time to re-balance the tree on insertions and removals. In other words, it guarantees that the height of the tree is still logarithmic, which will give us", "\"good\" balance condition has two properties: The height of a binary search tree meeting the condition is \u0398(log n). It takes O(log n) time to re-balance the tree on insertions and removals. In other words, it guarantees that the height of the tree is still logarithmic, which will give us logarithmic-time lookups, and the time spent re-balancing won't exceed the logarithmic time we would otherwise spend on an insertion or removal when the tree has logarithmic height. The cost won't outweigh the benefit. Coming up with a balance condition like this on our own is a tall task, but we can stand on the shoulders of the giants who came before us, with the definition above helping to guide us toward an understanding of whether we've found what we're looking for. A compromise: AVL trees There are a few well-known approaches for maintaining binary search trees in a state of near-balance that meets our notion of a \"good\" balance condition. One of them is called an AVL tree, which we'll explore here. Others, which are outside the scope of this course, include red-black trees (which meet our definition of \"good\") and splay trees (which don't always meet our definition of \"good\", but do meet it on an amortized basis), but we'll stick with the one solution to the problem for now. 3/18/25, 5:04 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ 4/12", "AVL trees AVL trees are what you might called \"nearly balanced\" binary search trees. While they certainly aren't as perfectly-balanced as possible, they nonetheless achieve the goals we've decided on: maintaining logarithmic height at no more than logarithmic cost. So, what makes a binary search tree \"nearly balanced\" enough to be considered an AVL tree? The core concept is embodied by something called the AVL property. We say that a node in a binary search tree has the AVL property if the heights of its left and right subtrees differ by no more than 1. In other words, we tolerate a certain amount of imbalance \u2014 heights of subtrees can be slightly different, but no more than that \u2014 in hopes that we can more efficiently maintain it. Since we're going to be comparing heights of subtrees, there's one piece of background we need to consider. Recall that the height of a tree is the length of its longest path. By definition, the height of a tree with just a root node (and empty subtrees) would then be zero. But what about a tree that's totally empty? To maintain a clear pattern, relative to other tree heights, we'll say that the height of an empty tree is -1. This means that a node with, say, a childless left child and no right child would still be considered balanced. This leads us, finally, to the definition of an AVL tree: An AVL tree is a binary search tree in which all nodes have the AVL property. Below are a few binary trees, two of which are AVL and two of which are not. The thing to keep in mind about AVL is that it's not a matter of squinting at a tree and deciding whether it \"looks\" balanced. There's a precise", "nodes have the AVL property. Below are a few binary trees, two of which are AVL and two of which are not. The thing to keep in mind about AVL is that it's not a matter of squinting at a tree and deciding whether it \"looks\" balanced. There's a precise definition, and the two trees above that don't meet that definition fail to meet it because they each have at least one node (marked in the diagrams by a dashed square) that doesn't have the AVL property. AVL trees, by definition, are required to meet the balance condition after every operation; every time you insert or remove a key, every node in the tree should have the AVL property. To meet that requirement, we need to restructure the tree periodically, essentially detecting and correcting imbalance whenever and wherever it happens. To do that, we need to rearrange the tree in ways that improve its shape without losing the essential ordering property of a binary search tree: smaller keys toward the left, larger ones toward the right. 3/18/25, 5:04 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ 5/12", "Rotations Re-balancing of AVL trees is achieved using what are called rotations, which, when used at the proper times, efficiently improve the shape of the tree by altering a handful of pointers. There are a few kinds of rotations; we should first understand how they work, then focus our attention on when to use them. The first kind of rotation is called an LL rotation, which takes the tree on the left and turns it into the tree on the right. The circle with A and B written in them are each a single node containing a single key; the triangles with T1, T2, and T3 written in them are arbitrary subtrees, which may be empty or may contain any number of nodes (but which are, themselves, binary search trees). It's important to remember that both of these trees \u2014 before and after \u2014 are binary search trees; the rotation doesn't harm the ordering of the keys in nodes, because the subtrees T1, T2, and T3 maintain the appropriate positions relative to the keys A and B: All keys in T1 are smaller than A. All keys in T2 are larger than A and smaller than B. All keys in T3 are larger than B. Performing this rotation would be a simple matter of adjusting a few pointers \u2014 notably, a constant number of pointers, no matter how many nodes are in the tree, which means that this rotation would run in \u0398(1) time: B's parent would now point to A where it used to point to B A's right child would now be B instead of the root of T2 B's left child would now be the root of T2 instead of A A second kind of rotation is an RR rotation, which makes a similar adjustment. 3/18/25, 5:04", "A where it used to point to B A's right child would now be B instead of the root of T2 B's left child would now be the root of T2 instead of A A second kind of rotation is an RR rotation, which makes a similar adjustment. 3/18/25, 5:04 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ 6/12", "Note that an RR rotation is the mirror image of an LL rotation. A third kind of rotation is an LR rotation, which makes an adjustment that's slightly more complicated. An LR rotation requires five pointer updates instead of three, but this is still a constant number of changes and runs in \u0398(1) time. Finally, there is an RL rotation, which is the mirror image of an LR rotation. 3/18/25, 5:04 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ 7/12", "Once we understand the mechanics of how rotations work, we're one step closer to understanding AVL trees. But these rotations aren't arbitrary; they're used specifically to correct imbalances that are detected after insertions or removals. An insertion algorithm Inserting a key into an AVL tree starts out the same way as insertion into a binary search tree: Perform a lookup. If you find the key already in the tree, you're done, because keys in a binary search tree must be unique. When the lookup terminates without the key being found, add a new node in the appropriate leaf position where the lookup ended. The problem is that adding the new node introduced the possibility of an imbalance. For example, suppose we started with this AVL tree: and then we inserted the key 35 into it. A binary search tree insertion would give us this as a result: 3/18/25, 5:04 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ 8/12", "But this resulting tree is not an AVL tree, because the node containing the key 40 does not have the AVL property, because the difference in the heights of its subtrees is 2. (Its left subtree has height 1, its right subtree \u2014 which is empty \u2014 has height -1.) What can we do about it? The answer lies in the following algorithm, which we perform after the normal insertion process: Work your way back up the tree from the position where you just added a node. (This could be quite simple if the insertion was done recursively.) Compare the heights of the left and right subtrees of each node. When they differ by more than 1, choose a rotation that will fix the imbalance. Note that comparing the heights of the left and right subtrees would be quite expensive if you didn't already know what they were. The solution to this problem is for each node to store its height (i.e., the height of the subtree rooted there). This can be cheaply updated after every insertion or removal as you unwind the recursion. The rotation is chosen considering the two links along the path below the node where the imbalance is, heading back down toward where you inserted a node. (If you were wondering where the names LL, RR, LR, and RL come from, this is the answer to that mystery.) If the two links are both to the left, perform an LL rotation rooted where the imbalance is. If the two links are both to the right, perform an RR rotation rooted where the imbalance is. If the first link is to the left and the second is to the right, perform an LR rotation rooted where the imbalance is. If the first link is to the right", "If the two links are both to the right, perform an RR rotation rooted where the imbalance is. If the first link is to the left and the second is to the right, perform an LR rotation rooted where the imbalance is. If the first link is to the right and the second is to the left, perform an RL rotation rooted where the imbalance is. It can be shown that any one of these rotations \u2014 LL, RR, LR, or RL \u2014 will correct any imbalance brought on by inserting a key. In this case, we'd perform an LR rotation \u2014 the first two links leading from 40 down toward 35 are a Left and a Right \u2014 rooted at 40, which would correct the imbalance, and the tree would be rearranged to look like this: 3/18/25, 5:04 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ 9/12", "Compare this to the diagram describing an LR rotation: The node containing 40 is C The node containing 30 is A The node containing 35 is B The (empty) left subtree of the node containing 30 is T1 The (empty) left subtree of the node containing 35 is T2 The (empty) right subtree of the node containing 35 is T3 The (empty) right subtree of the node containing 40 is T4 After the rotation, we see what we'd expect: The node B, which in our example contained 35, is now the root of the newly-rotated subtree The node A, which in our example contained 30, is now the left child of the root of the newly-rotated subtree The node C, which in our example contained 40, is now the right child of the root of the newly-rotated subtree The four subtrees T1, T2, T3, and T4 were all empty, so they are still empty. Note, too, that the tree is more balanced after the rotation than it was before. This is no accident; a single rotation (LL, RR, LR, or RL) is all that's necessary to correct an imbalance introduced by the insertion algorithm. A removal algorithm Removals are somewhat similar to insertions, in the sense that you would start with the usual binary search tree removal algorithm, then find and correct imbalances while the recursion unwinds. The key difference is that removals can require more than one rotation to correct imbalances, but will still only require rotations on the path back up to the root from where the removal occurred \u2014 so, generally, O(log n) rotations. Asymptotic analysis The key question here is What is the height of an AVL tree with n nodes? If the answer is \u0398(log n), then we can be certain that lookups, insertions, and", "path back up to the root from where the removal occurred \u2014 so, generally, O(log n) rotations. Asymptotic analysis The key question here is What is the height of an AVL tree with n nodes? If the answer is \u0398(log n), then we can be certain that lookups, insertions, and removals will take O(log n) time. How can we be so sure? 3/18/25, 5:04 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ 10/12", "Lookups would be O(log n) because they're the same as they are in a binary search tree that doesn't have the AVL property. If the height of the tree is \u0398(log n), lookups will run in O(log n) time. Insertions and removals, despite being slightly more complicated in an AVL tree, do their work by traversing a single path in the tree \u2014 potentially all the way down to a leaf position, then all the way back up. If the length of the longest path \u2014 that's what the height of a tree is! \u2014 is \u0398(log n), then we know that none of these paths is longer than that, so insertions and removals will take O(log n) time. So we're left with that key question. What is the height of an AVL tree with n nodes? (If you're not curious, you can feel free to just assume this; if you want to know more, keep reading.) What is the height of an AVL tree with n nodes? (Optional) The answer revolves around noting how many nodes, at minimum, could be in a binary search tree of height n and still have it be an AVL tree. It turns out AVL trees of height n \u2265 2 that have the minimum number of nodes in them all share a similar property: The AVL tree with height h \u2265 2 with the minimum number of nodes consists of a root node with two subtrees, one of which is an AVL tree with height h \u2212 1 with the minimum number of nodes, the other of which is an AVL tree with height h \u2212 2 with the minimum number of nodes. Given that observation, we can write a recurrence that describes the number of nodes, at minimum, in an AVL tree", "with height h \u2212 1 with the minimum number of nodes, the other of which is an AVL tree with height h \u2212 2 with the minimum number of nodes. Given that observation, we can write a recurrence that describes the number of nodes, at minimum, in an AVL tree of height h. M(0) = 1 When height is 0, minimum number of nodes is 1 (a root node with no children) M(1) = 2 When height is 1, minimum number of nodes is 2 (a root node with one child and not the other) M(h) = 1 + M(h - 1) + M(h - 2) While the repeated substitution technique we learned previously isn't a good way to try to solve this particular recurrence, we can prove something interesting quite easily. We know for sure that AVL trees with larger heights have a bigger minimum number of nodes than AVL trees with smaller heights \u2014 that's fairly self-explanatory \u2014 which means that we can be sure that 1 + M(h \u2212 1) \u2265 M(h \u2212 2). Given that, we can conclude the following: M(h) \u2265 2M(h - 2) We can then use the repeated substitution technique to determine a lower bound for this recurrence: M(h) \u2265 2M(h - 2) \u2265 2(2M(h - 4)) \u2265 4M(h - 4) \u2265 4(2M(h - 6)) \u2265 8M(h - 6) ... \u2265 2jM(h - 2j) We could prove this by induction on j, but we'll accept it on faith let j = h/2 \u2265 2h/2M(h - h) \u2265 2h/2M(0) M(h) \u2265 2h/2 3/18/25, 5:04 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ 11/12", "2h/2M(h - h) \u2265 2h/2M(0) M(h) \u2265 2h/2 3/18/25, 5:04 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ 11/12", "So, we've shown that the minimum number of nodes that can be present in an AVL tree of height h is at least 2h/2. In reality, it's actually more than that, but this gives us something useful to work with; we can use this result to figure out what we're really interested in, which is the opposite: what is the height of an AVL tree with n nodes? M(h) \u2265 2h/2 log2M(h) \u2265 h/2 2 log2M(h) \u2265 h Finally, we see that, for AVL trees of height h with the minimum number of nodes, the height is no more than 2 log2n, where n is the number of nodes in the tree. For AVL trees with more than the minimum number of nodes, the relationship between the number of nodes and the height is even better, though, for reasons we've seen previously, we know that the relationship between the number of nodes and the height of a binary tree can never be better than logarithmic. So, ultimately, we see that the height of an AVL tree with n nodes is \u0398(log n). (In reality, it turns out that the bound is lower than 2 log2n; it's something more akin to about 1.44 log2n, even for AVL trees with the minimum number of nodes, though the proof of that is more involved and doesn't change the asymptotic result.) 3/18/25, 5:04 PM ICS 46 Spring 2022, Notes and Examples: AVL Trees https://ics.uci.edu/~thornton/ics46/Notes/AVLTrees/ 12/12", "Chapter 12: Binary Search Trees A binary search tree is a binary tree with a special property called the BST-property, which is given as follows: \u22c6 For all nodes x and y, if y belongs to the left subtree of x, then the key at y is less than the key at x, and if y belongs to the right subtree of x, then the key at y is greater than the key at x. We will assume that the keys of a BST are pairwise distinct. Each node has the following attributes: \u2022 p, left, and right, which are pointers to the parent, the left child, and the right child, respectively, and \u2022 key, which is key stored at the node. 1", "An example 4 2 3 6 5 12 9 8 11 15 19 20 7 2", "Traversal of the Nodes in a BST By \u201ctraversal\u201d we mean visiting all the nodes in a graph. Traversal strategies can be speci\ufb01ed by the ordering of the three objects to visit: the current node, the left subtree, and the right subtree. We assume the the left subtree always comes before the right subtree. Then there are three strategies. 1. Inorder. The ordering is: the left subtree, the current node, the right subtree. 2. Preorder. The ordering is: the current node, the left subtree, the right subtree. 3. Postorder. The ordering is: the left subtree, the right subtree, the current node. 3", "Inorder Traversal Pseudocode This recursive algorithm takes as the input a pointer to a tree and executed inorder traversal on the tree. While doing traversal it prints out the key of each node that is visited. Inorder-Walk(x) 1: if x = nil then return 2: Inorder-Walk(left[x]) 3: Print key[x] 4: Inorder-Walk(right[x]) We can write a similar pseudocode for preorder and postorder. 4", "preorder postorder inorder 2 1 3 1 1 2 2 3 3 4 2 3 6 5 12 9 8 11 15 19 20 7 What is the outcome of inorder traversal on this BST? How about postorder traversal and preorder traversal? 5", "Inorder traversal gives: 2, 3, 4, 5, 6, 7, 8 , 9, 11, 12, 15, 19, 20. Preorder traversal gives: 7, 4, 2, 3, 6, 5, 12, 9, 8, 11, 19, 15, 20. Postorder traversal gives: 3, 2, 5, 6, 4, 8, 11, 9, 15, 20, 19, 12, 7. So, inorder travel on a BST \ufb01nds the keys in nondecreasing order! 6", "Operations on BST 1. Searching for a key We assume that a key and the subtree in which the key is searched for are given as an input. We\u2019ll take the full advantage of the BST-property. Suppose we are at a node. If the node has the key that is being searched for, then the search is over. Otherwise, the key at the current node is either strictly smaller than the key that is searched for or strictly greater than the key that is searched for. If the former is the case, then by the BST property, all the keys in th left subtree are strictly less than the key that is searched for. That means that we do not need to search in the left subtree. Thus, we will examine only the right subtree. If the latter is the case, by symmetry we will examine only the right subtree. 7", "Algorithm Here k is the key that is searched for and x is the start node. BST-Search(x, k) 1: y \u2190x 2: while y \u0338= nil do 3: if key[y] = k then return y 4: else if key[y] < k then y \u2190right[y] 5: else y \u2190left[y] 6: return (\u201cNOT FOUND\u201d) 8", "An Example search for 8 7 4 2 6 9 13 11 NIL What is the running time of search? 9", "2. The Maximum and the Minimum To \ufb01nd the minimum identify the leftmost node, i.e. the farthest node you can reach by following only left branches. To \ufb01nd the maximum identify the rightmost node, i.e. the farthest node you can reach by following only right branches. BST-Minimum(x) 1: if x = nil then return (\u201cEmpty Tree\u201d) 2: y \u2190x 3: while left[y] \u0338= nil do y \u2190left[y] 4: return (key[y]) BST-Maximum(x) 1: if x = nil then return (\u201cEmpty Tree\u201d) 2: y \u2190x 3: while right[y] \u0338= nil do y \u2190right[y] 4: return (key[y]) 10", "3. Insertion Suppose that we need to insert a node z such that k = key[z]. Using binary search we \ufb01nd a nil such that replacing it by z does not break the BST-property. 11", "BST-Insert(x, z, k) 1: if x = nil then return \u201cError\u201d 2: y \u2190x 3: while true do { 4: if key[y] < k 5: then z \u2190left[y] 6: else z \u2190right[y] 7: if z = nil break 8: } 9: if key[y] > k then left[y] \u2190z 10: else right[p[y]] \u2190z 12", "4. The Successor and The Predecessor The successor (respectively, the predecessor) of a key k in a search tree is the smallest (respectively, the largest) key that belongs to the tree and that is strictly greater than (respectively, less than) k. The idea for \ufb01nding the successor of a given node x. \u2022 If x has the right child, then the successor is the minimum in the right subtree of x. \u2022 Otherwise, the successor is the parent of the farthest node that can be reached from x by following only right branches backward. 13", "An Example 4 2 3 6 5 12 9 8 11 15 19 20 7 25 23 14", "Algorithm BST-Successor(x) 1: if right[x] \u0338= nil then 2: { y \u2190right[x] 3: while left[y] \u0338= nil do y \u2190left[y] 4: return (y) } 5: else 6: { y \u2190x 7: while right[p[x]] = x do y \u2190p[x] 8: if p[x] \u0338= nil then return (p[x]) 9: else return (\u201cNO SUCCESSOR\u201d) } 15", "The predecessor can be found similarly with the roles of left and right exchanged and with the roles of maximum and minimum exchanged. For which node is the successor unde\ufb01ned? What is the running time of the successor algorithm? 16", "5. Deletion Suppose we want to delete a node z. 1. If z has no children, then we will just replace z by nil. 2. If z has only one child, then we will promote the unique child to z\u2019s place. 3. If z has two children, then we will identify z\u2019s successor. Call it y. The successor y either is a leaf or has only the right child. Promote y to z\u2019s place. Treat the loss of y using one of the above two solutions. 17", "10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 9 13 11 8 4 2 3 5 6 1 3 2 4 7 9 13 11 8 5 6 9 13 11 8 7 10 10 10 5 6 1 3 2 4 7 9 13 11 8 5 6 1 3 2 4 13 11 9 10 18", "Algorithm This algorithm deletes z from BST T. BST-Delete(T, z) 1: if left[z] = nil or right[z] = nil 2: then y \u2190z 3: else y \u2190BST-Successor(z) 4: \u0003 y is the node that\u2019s actually removed. 5: \u0003 Here y does not have two children. 6: if left[y] \u0338= nil 7: then x \u2190left[y] 8: else x \u2190right[y] 9: \u0003 x is the node that\u2019s moving to y\u2019s position. 10: if x \u0338= nil then p[x] \u2190p[y] 11: \u0003 p[x] is reset If x isn\u2019t NIL. 12: \u0003 Resetting is unnecessary if x is NIL. 19", "Algorithm (cont\u2019d) 13: if p[y] = nil then root[T] \u2190x 14: \u0003 If y is the root, then x becomes the root. 15: \u0003 Otherwise, do the following. 16: else if y = left[p[y]] 17: then left[p[y]] \u2190x 18: \u0003 If y is the left child of its parent, then 19: \u0003 Set the parent\u2019s left child to x. 20: else right[p[y]] \u2190x 21: \u0003 If y is the right child of its parent, then 22: \u0003 Set the parent\u2019s right child to x. 23: if y \u0338= z then 24: { key[z] \u2190key[y] 25: Move other data from y to z } 27: return (y) 20", "Summary of E\ufb03ciency Analysis Theorem A On a binary search tree of height h, Search, Minimum, Maximum, Successor, Predecessor, Insert, and Delete can be made to run in O(h) time. 21", "Randomly built BST Suppose that we insert n distinct keys into an initially empty tree. Assuming that the n! permutations are equally likely to occur, what is the average height of the tree? To study this question we consider the process of constructing a tree T by inserting in order randomly selected n distinct keys to an initially empty tree. Here the actually values of the keys do not matter. What matters is the position of the inserted key in the n keys. 22", "The Process of Construction So, we will view the process as follows: A key x from the keys is selected uniformly at random and is inserted to the tree. Then all the other keys are inserted. Here all the keys greater than x go into the right subtree of x and all the keys smaller than x go into the left subtree. Thus, the height of the tree thus constructed is one plus the larger of the height of the left subtree and the height of the right subtree. 23", "Random Variables n = number of keys Xn = height of the tree of n keys Yn = 2Xn. We want an upper bound on E[Yn]. For n \u22652, we have E[Yn] = 1 n \uf8eb \uf8ed n X i=1 2E[max{Yi\u22121, Yn\u2212i}] \uf8f6 \uf8f8. E[max{Yi\u22121, Yn\u2212i}] \u2264 E[Yi\u22121 + Yn\u2212i] \u2264 E[Yi\u22121] + E[Yn\u2212i] Collecting terms: E[Yn] \u22644 n n\u22121 X i=1 E[Yi]. 24", "Analysis We claim that for all n \u22651 E[Yn] \u22641 4 \u0010n+3 3 \u0011 . We prove this by induction on n. Base case: E[Y1] = 20 = 1. Induction step: We have E[Yn] \u22644 n n\u22121 X i=1 E[Yi] Using the fact that n\u22121 X i=0 \u0010i + 3 3 \u0011 = \u0010n + 3 4 \u0011 E[Yn] \u22644 n \u00b7 1 4 \u00b7 \u0010n + 3 4 \u0011 E[Yn] \u22641 4 \u00b7 \u0010n + 3 3 \u0011 25", "Jensen\u2019s inequality A function f is convex if for all x and y, x < y, and for all \u03bb, 0 \u2264\u03bb \u22641, f(\u03bbx + (1 \u2212\u03bb)y) \u2264\u03bbf(x) + (1 \u2212\u03bb)f(y) Jensen\u2019s inequality states that for all random variables X and for all convex function f f(E[X]) \u2264E[f(X)]. Let this X be Xn and f(x) = 2x. Then E[f(X)] = E[Yn]. So, we have 2E[Xn] \u22641 4 \u0010n + 3 3 \u0011 . The right-hand side is at most (n + 3)3. By taking the log of both sides, we have E[Xn] = O(log n). Thus the average height of a randomly build BST is O(log n). 26", "12.6. B-Trees 12.6.1. B-Trees This module presents the B-tree. B-trees are usually attributed to R. Bayer and E. McCreight who described the B-tree in a 1972 paper. By 1979, B-trees had replaced virtually all large-file access methods other than hashing. B-trees, or some variant of B-trees, are the standard file organization for applications requiring insertion, deletion, and key range searches. They are used to implement most modern file systems. B-trees address effectively all of the major problems encountered when implementing disk-based search trees: 1. The B-tree is shallow, in part because the tree is always height balanced (all leaf nodes are at the same level), and in part because the branching factor is quite high. So only a small number of disk blocks are accessed to reach a given record. 2. Update and search operations affect only those disk blocks on the path from the root to the leaf node containing the query record. The fewer the number of disk blocks affected during an operation, the less disk I/O is required. 3. B-trees keep related records (that is, records with similar key values) on the same disk block, which helps to minimize disk I/O on range searches. 4. B-trees guarantee that every node in the tree will be full at least to a certain minimum percentage. This improves space efficiency while reducing the typical number of disk fetches necessary during a search or update operation. A B-tree of order is defined to have the following shape properties: The root is either a leaf or has at least two children. Each internal node, except for the root, has between and children. All leaves are at the same level in the tree, so the tree is always height balanced. The B-tree is a generalization of the 2-3 tree. Put another way, a", "either a leaf or has at least two children. Each internal node, except for the root, has between and children. All leaves are at the same level in the tree, so the tree is always height balanced. The B-tree is a generalization of the 2-3 tree. Put another way, a 2-3 tree is a B-tree of order three. Normally, the size of a node in the B-tree is chosen to fill a disk block. A B-tree node implementation typically allows 100 or more children. Thus, a B-tree node is equivalent to a disk block, and a \u201cpointer\u201d value stored in the tree is actually the number of the block containing the child node (usually interpreted as an offset from the beginning of the corresponding disk file). In a typical application, the B-tree\u2019s access to the disk file will be managed using a buffer pool and a block-replacement scheme such as LRU. Figure 12.6.1 shows a B-tree of order four. Each node contains up to three keys, and internal nodes have up to four children. m \u2308m/2\u2309 m 3/18/25, 5:05 PM 12.6. B-Trees \u2014 CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 1/13", "Figure 12.6.1: A B-tree of order four. Search in a B-tree is a generalization of search in a 2-3 tree. It is an alternating two-step process, beginning with the root node of the B-tree. 1. Perform a binary search on the records in the current node. If a record with the search key is found, then return that record. If the current node is a leaf node and the key is not found, then report an unsuccessful search. 2. Otherwise, follow the proper branch and repeat the process. For example, consider a search for the record with key value 47 in the tree of Figure 12.6.1. The root node is examined and the second (right) branch taken. After examining the node at level 1, the third branch is taken to the next level to arrive at the leaf node containing a record with key value 47. B-tree insertion is a generalization of 2-3 tree insertion. The first step is to find the leaf node that should contain the key to be inserted, space permitting. If there is room in this node, then insert the key. If there is not, then split the node into two and promote the middle key to the parent. If the parent becomes full, then it is split in turn, and its middle key promoted. Note that this insertion process is guaranteed to keep all nodes at least half full. For example, when we attempt to insert into a full internal node of a B- tree of order four, there will now be five children that must be dealt with. The node is split into two nodes containing two keys each, thus retaining the B- tree property. The middle of the five children is promoted to its parent. 12.6.1.1. B+ Trees The previous section mentioned that", "of order four, there will now be five children that must be dealt with. The node is split into two nodes containing two keys each, thus retaining the B- tree property. The middle of the five children is promoted to its parent. 12.6.1.1. B+ Trees The previous section mentioned that B-trees are universally used to implement large-scale disk-based systems. Actually, the B-tree as described in the previous section is almost never implemented. What is most commonly implemented is a variant of the B-tree, called the tree. When greater efficiency is required, a more complicated variant known as the tree is used. 24 15 20 33 45 48 10 12 18 21 23 30 30 38 47 50 52 60 B+ B\u2217 3/18/25, 5:05 PM 12.6. B-Trees \u2014 CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 2/13", "Consider again the linear index. When the collection of records will not change, a linear index provides an extremely efficient way to search. The problem is how to handle those pesky inserts and deletes. We could try to keep the core idea of storing a sorted array-based list, but make it more flexible by breaking the list into manageable chunks that are more easily updated. How might we do that? First, we need to decide how big the chunks should be. Since the data are on disk, it seems reasonable to store a chunk that is the size of a disk block, or a small multiple of the disk block size. If the next record to be inserted belongs to a chunk that hasn\u2019t filled its block then we can just insert it there. The fact that this might cause other records in that chunk to move a little bit in the array is not important, since this does not cause any extra disk accesses so long as we move data within that chunk. But what if the chunk fills up the entire block that contains it? We could just split it in half. What if we want to delete a record? We could just take the deleted record out of the chunk, but we might not want a lot of near-empty chunks. So we could put adjacent chunks together if they have only a small amount of data between them. Or we could shuffle data between adjacent chunks that together contain more data. The big problem would be how to find the desired chunk when processing a record with a given key. Perhaps some sort of tree-like structure could be used to locate the appropriate chunk. These ideas are exactly what motivate the tree. The tree is essentially a", "together contain more data. The big problem would be how to find the desired chunk when processing a record with a given key. Perhaps some sort of tree-like structure could be used to locate the appropriate chunk. These ideas are exactly what motivate the tree. The tree is essentially a mechanism for managing a sorted array-based list, where the list is broken into chunks. The most significant difference between the tree and the BST or the standard B-tree is that the tree stores records only at the leaf nodes. Internal nodes store key values, but these are used solely as placeholders to guide the search. This means that internal nodes are significantly different in structure from leaf nodes. Internal nodes store keys to guide the search, associating each key with a pointer to a child tree node. Leaf nodes store actual records, or else keys and pointers to actual records in a separate disk file if the tree is being used purely as an index. Depending on the size of a record as compared to the size of a key, a leaf node in a tree of order might have enough room to store more or less than records. The requirement is simply that the leaf nodes store enough records to remain at least half full. The leaf nodes of a tree are normally linked together to form a doubly linked list. Thus, the entire collection of records can be traversed in sorted order by visiting all the leaf nodes on the linked list. Here is a Java-like pseudocode representation for the tree node interface. Leaf node and internal node subclasses would implement this interface. /** Interface for B+ Tree nodes */ public interface BPNode<Key,E> { public boolean isLeaf(); public int numrecs(); public Key[] keys(); } An important implementation detail", "on the linked list. Here is a Java-like pseudocode representation for the tree node interface. Leaf node and internal node subclasses would implement this interface. /** Interface for B+ Tree nodes */ public interface BPNode<Key,E> { public boolean isLeaf(); public int numrecs(); public Key[] keys(); } An important implementation detail to note is that while Figure 12.6.1 shows internal nodes containing three keys and four pointers, class BPNode is slightly different in that it stores key/pointer pairs. Figure 12.6.1 shows the tree as it is traditionally drawn. To simplify implementation in practice, nodes really do associate a key with each pointer. Each internal node should be assumed to hold in the leftmost position an additional key that is less than or equal to any possible key value in the node\u2019s leftmost subtree. tree implementations typically store an additional dummy record in the leftmost leaf node whose key value is less than any legal key value. Let\u2019s see in some detail how the simplest tree works. This would be the \u201c tree\u201d, or a tree of order 3. B+ B+ B+ B+ B+ B+ B+ m m B+ B+ B+ B+ B+ 2 \u22123+ B+ 3/18/25, 5:05 PM 12.6. B-Trees \u2014 CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 3/13", "Figure 12.6.2: An example of building a tree Next, let\u2019s see how to search. 1 / 28 << < > >> Example 2-3+ Tree Visualization: Insert 2 \u22123+ 1 / 10 << < > >> Example 2-3+ Tree Visualization: Search 46 65 3/18/25, 5:05 PM 12.6. B-Trees \u2014 CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 4/13", "Figure 12.6.3: An example of searching a tree Finally, let\u2019s see an example of deleting from the tree Figure 12.6.4: An example of deleting from a tree Now, let\u2019s extend these ideas to a tree of higher order. 15 J 22 X 52 B 33 65 S 71 W 89 M 71 33 O 46 H 47 L 52 2 \u22123+ 2 \u22123+ 1 / 33 << < > >> Example 2-3+ Tree Visualization: Delete 15 J 71 W 89 M 22 65 S 70 F 51 B 52 T 71 46 65 46 H 47 L 22 X 33 O 51 2 \u22123+ B+ 3/18/25, 5:05 PM 12.6. B-Trees \u2014 CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 5/13", "trees are exceptionally good for range queries. Once the first record in the range has been found, the rest of the records with keys in the range can be accessed by sequential processing of the remaining records in the first node, and then continuing down the linked list of leaf nodes as far as necessary. Figure illustrates the tree. Figure 12.6.5: An example of search in a B+ tree of order four. Internal nodes must store between two and four children. Search in a tree is nearly identical to search in a regular B-tree, except that the search must always continue to the proper leaf node. Even if the search-key value is found in an internal node, this is only a placeholder and does not provide access to the actual record. Here is a pseudocode sketch of the tree search algorithm. private E findhelp(BPNode<Key,E> rt, Key k) { int currec = binaryle(rt.keys(), rt.numrecs(), k); if (rt.isLeaf()) { if ((((BPLeaf<Key,E>)rt).keys())[currec] == k) { return ((BPLeaf<Key,E>)rt).recs(currec); B+ B+ 1 / 10 << < > >> Example B+ Tree Visualization: Search in a tree of degree 4 10 S 18 E 40 Q 55 F 25 40 77 A 89 B 98 A 127 V 25 T 39 F 98 77 B+ B+ 3/18/25, 5:05 PM 12.6. B-Trees \u2014 CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 6/13", "} else { return null; } } else{ return findhelp(((BPInternal<Key,E>)rt).pointers(currec), k); } } tree insertion is similar to B-tree insertion. First, the leaf that should contain the record is found. If is not full, then the new record is added, and no other tree nodes are affected. If is already full, split it in two (dividing the records evenly among the two nodes) and promote a copy of the least- valued key in the newly formed right node. As with the 2-3 tree, promotion might cause the parent to split in turn, perhaps eventually leading to splitting the root and causing the tree to gain a new level. tree insertion keeps all leaf nodes at equal depth. Figure illustrates the insertion process through several examples. Figure 12.6.6: An example of building a B+ tree of order four. B+ L L B+ L B+ B+ 1 / 42 << < > >> Example B+ Tree Visualization: Insert into a tree of degree 4 3/18/25, 5:05 PM 12.6. B-Trees \u2014 CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 7/13", "Here is a a Java-like pseudocode sketch of the tree insert algorithm. private BPNode<Key,E> inserthelp(BPNode<Key,E> rt, Key k, E e) { BPNode<Key,E> retval; if (rt.isLeaf()) { // At leaf node: insert here return ((BPLeaf<Key,E>)rt).add(k, e); } // Add to internal node int currec = binaryle(rt.keys(), rt.numrecs(), k); BPNode<Key,E> temp = inserthelp( ((BPInternal<Key,E>)root).pointers(currec), k, e); if (temp != ((BPInternal<Key,E>)rt).pointers(currec)) { return ((BPInternal<Key,E>)rt). add((BPInternal<Key,E>)temp); } else{ return rt; } } Here is an exercise to see if you get the basic idea of tree insertion. B+ B+ 3/18/25, 5:05 PM 12.6. B-Trees \u2014 CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 8/13", "B+ Tree Insertion Instructions: In this exercise your job is to insert the values from the stack to the B+ tree. Search for the leaf node where the topmost value of the stack should be inserted, and click on that node. The exercise will take care of the rest. Continue this procedure until you have inserted all the values in the stack. Undo Reset Model Answer Grade 76 93 20 33 51 62 12 30 24 68 77 97 39 53 98 94 56 16 3/18/25, 5:05 PM 12.6. B-Trees \u2014 CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 9/13", "To delete record from the tree, first locate the leaf that contains . If is more than half full, then we need only remove , leaving still at least half full. This is demonstrated by Figure . Figure 12.6.7: An example of deletion in a B+ tree of order four. If deleting a record reduces the number of records in the node below the minimum threshold (called an underflow), then we must do something to keep the node sufficiently full. The first choice is to look at the node\u2019s adjacent siblings to determine if they have a spare record that can be used to fill the gap. If so, then enough records are transferred from the sibling so that both nodes have about the same number of records. This is done so as to delay as long as possible the next time when a delete causes this node to underflow again. This process might require that the parent node has its placeholder key value revised to reflect the true first key value in each node. If neither sibling can lend a record to the under-full node (call it ), then must give its records to a sibling and be removed from the tree. There is certainly room to do this, because the sibling is at most half full (remember that it had no records to contribute to the current node), and has become less than half full because it is under-flowing. This merge process combines two subtrees of the parent, which might cause it to underflow in turn. If the last two children of the root merge together, then the tree loses a level. Here is a Java-like pseudocode for the tree delete algorithm. R B+ L R L R L 1 / 23 << < > >> Example B+", "which might cause it to underflow in turn. If the last two children of the root merge together, then the tree loses a level. Here is a Java-like pseudocode for the tree delete algorithm. R B+ L R L R L 1 / 23 << < > >> Example B+ Tree Visualization: Delete from a tree of degree 4 5 F 10 S 44 Q 48 E 12 44 67 A 88 B 58 A 60 F 12 V 27 T 67 58 N N N B+ 3/18/25, 5:05 PM 12.6. B-Trees \u2014 CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 10/13", "/** Delete a record with the given key value, and return true if the root underflows */ private boolean removehelp(BPNode<Key,E> rt, Key k) { int currec = binaryle(rt.keys(), rt.numrecs(), k); if (rt.isLeaf()) { if (((BPLeaf<Key,E>)rt).keys()[currec] == k) { return ((BPLeaf<Key,E>)rt).delete(currec); } else { return false; } } else{ // Process internal node if (removehelp(((BPInternal<Key,E>)rt).pointers(currec), k)) { // Child will merge if necessary return ((BPInternal<Key,E>)rt).underflow(currec); } else { return false; } } } The tree requires that all nodes be at least half full (except for the root). Thus, the storage utilization must be at least 50%. This is satisfactory for many implementations, but note that keeping nodes fuller will result both in less space required (because there is less empty space in the disk file) and in more efficient processing (fewer blocks on average will be read into memory because the amount of information in each block is greater). Because B-trees have become so popular, many algorithm designers have tried to improve B-tree performance. One method for doing so is to use the tree variant known as the tree. The tree is identical to the tree, except for the rules used to split and merge nodes. Instead of splitting a node in half when it overflows, the tree gives some records to its neighboring sibling, if possible. If the sibling is also full, then these two nodes split into three. Similarly, when a node underflows, it is combined with its two siblings, and the total reduced to two nodes. Thus, the nodes are always at least two thirds full. [1] Finally, here is an example of building a B+ Tree of order five. You can compare this to the example above of building a tree of order four with the same records. B+ B+ B\u2217 B\u2217 B+ B\u2217 1 /", "the nodes are always at least two thirds full. [1] Finally, here is an example of building a B+ Tree of order five. You can compare this to the example above of building a tree of order four with the same records. B+ B+ B\u2217 B\u2217 B+ B\u2217 1 / 33 << < > >> Example B+ Tree Visualization: Insert into a tree of degree 5 3/18/25, 5:05 PM 12.6. B-Trees \u2014 CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 11/13", "Figure 12.6.8: An example of building a B+ tree of degree 5 Click here for a visualization that will let you construct and interact with a tree. This visualization was written by David Galles of the University of San Francisco as part of his Data Structure Visualizations package. [1] This concept can be extended further if higher space utilization is required. However, the update routines become much more complicated. I once worked on a project where we implemented 3-for-4 node split and merge routines. This gave better performance than the 2-for-3 node split and merge routines of the tree. However, the spitting and merging routines were so complicated that even their author could no longer understand them once they were completed! 12.6.1.2. B-Tree Analysis The asymptotic cost of search, insertion, and deletion of records from B-trees, trees, and trees is where is the total number of records in the tree. However, the base of the log is the (average) branching factor of the tree. Typical database applications use extremely high branching factors, perhaps 100 or more. Thus, in practice the B-tree and its variants are extremely shallow. As an illustration, consider a tree of order 100 and leaf nodes that contain up to 100 records. A B- tree with height one (that is, just a single leaf node) can have at most 100 records. A tree with height two (a root internal node whose children are leaves) must have at least 100 records (2 leaves with 50 records each). It has at most 10,000 records (100 leaves with 100 records each). A tree with height three must have at least 5000 records (two B+ B\u2217 B+ B\u2217 \u0398(log n) n B+ B+ B+ B+ 3/18/25, 5:05 PM 12.6. B-Trees \u2014 CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 12/13", "has at most 10,000 records (100 leaves with 100 records each). A tree with height three must have at least 5000 records (two B+ B\u2217 B+ B\u2217 \u0398(log n) n B+ B+ B+ B+ 3/18/25, 5:05 PM 12.6. B-Trees \u2014 CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 12/13", "second-level nodes with 50 children containing 50 records each) and at most one million records (100 second-level nodes with 100 full children each). A tree with height four must have at least 250,000 records and at most 100 million records. Thus, it would require an extremely large database to generate a tree of more than height four. The tree split and insert rules guarantee that every node (except perhaps the root) is at least half full. So they are on average about 3/4 full. But the internal nodes are purely overhead, since the keys stored there are used only by the tree to direct search, rather than store actual data. Does this overhead amount to a significant use of space? No, because once again the high fan-out rate of the tree structure means that the vast majority of nodes are leaf nodes. A K-ary tree has approximately of its nodes as internal nodes. This means that while half of a full binary tree\u2019s nodes are internal nodes, in a tree of order 100 probably only about of its nodes are internal nodes. This means that the overhead associated with internal nodes is very low. We can reduce the number of disk fetches required for the B-tree even more by using the following methods. First, the upper levels of the tree can be stored in main memory at all times. Because the tree branches so quickly, the top two levels (levels 0 and 1) require relatively little space. If the B-tree is only height four, then at most two disk fetches (internal nodes at level two and leaves at level three) are required to reach the pointer to any given record. A buffer pool could be used to manage nodes of the B-tree. Several nodes of the tree would typically be", "is only height four, then at most two disk fetches (internal nodes at level two and leaves at level three) are required to reach the pointer to any given record. A buffer pool could be used to manage nodes of the B-tree. Several nodes of the tree would typically be in main memory at one time. The most straightforward approach is to use a standard method such as LRU to do node replacement. However, sometimes it might be desirable to \u201clock\u201d certain nodes such as the root into the buffer pool. In general, if the buffer pool is even of modest size (say at least twice the depth of the tree), no special techniques for node replacement will be required because the upper-level nodes will naturally be accessed frequently. B+ B+ B+ 1/K B+ 1/75 3/18/25, 5:05 PM 12.6. B-Trees \u2014 CS3 Data Structures & Algorithms https://opendsa-server.cs.vt.edu/ODSA/Books/CS3/html/BTree.html#id2 13/13", "CS 3200 The Relational Model of Data Mark Fontenot, PhD Northeastern University These notes are based upon and adapted via the generosity of Prof. Nate Derbinsky.", "Reminders - Make sure you can see the course on Gradescope, Campuswire, and Slack - Mini HW00 - Early EC Deadline: Sunday @ 11:59 pm - Regular Deadline: Tuesday @ 11:59 pm 2", "Applications and Data \u25cf Many applications are data intensive compared to compute intensive. \u25cf Many apps need to\u2026 \u25cb store data for itself or another app to use in the future (databases) \u25cb remember the results of prior expensive operations (caches) \u25cb allow users to search for data e\ufb03ciently (indexes) \u25cb send messages with data to another application to handle (stream processing) \u25cb periodically process a large accumulation of data (batch processing) From Kleppmann, Designing Data Intensive Applications, O\u2019Reilly, 2017. 3", "Stores of data? 4", "What is a database? \u25cfStructured collection of related data \u25cbusually related to something in the real world \u25cbusually created\u2026 \u25a0 for a speci\ufb01c group of users \u25a0 to help these users perform some kind of tasks \u25a0 to hopefully complete those tasks with some performance, redundancy, concurrency, and/or security considerations in mind 5", "Notion \u25cfIntended users? \u25cfIntended tasks? \u25cfConsiderations of \u25cbPerformance? \u25cbConcurrency? \u25cbRedundancy? \u25cbSecurity? 6", "Database Management Systems (DBMS) \u25cfSoftware that allow the creation and maintenance of databases \u25cbSupport the encoding of some type of structure for the data \u25cbPersists the data \u25cbSupport adding new data and updating existing data \u25cbProtects against failures and unauthorized access 7", "Some Categories of DBMSs Document-Oriented Databases - Organizes and queries data based on the concept of a \u201cdocument\u201d often in JSON - usually considered semi-structured Graph Databases - Organizes data by nodes, edges, labels - Query about paths between nodes and node relationships 8", "Some Categories of DBMSs Key-Value Databases - Everything is a key/value pair - Based on associative array Spatial Databases - Stores data related to 2D/3D locations - Query example: are 2 cars about to collide? 9", "Some Categories of DBMSs Vector Databases - unit of storage is a vector represent high-dimensional data - highly performant similarity searches - used extensively in LLMs 10", "The category for this course\u2026 \u25cfRelational Database Management Systems \u25cbBased on storing data in tables and connections between those tables \u25cbOriginal concept developed in early 70s by EF Codd and colleagues 11", "Relational Model of Data: Overview 12", "The Relational Database: Relation/Table Image borrowed from this Medium post. Students Relation/Table Name Attributes/Columns Rows/ Tuples 13 Relation - the core construct in a relational database; collection of tuples with each tuple having values for a \ufb01xed number of attributes/\ufb01elds. Relation Schema - represents the attributes and their data types for a particular relation Relation Instance - represents the state of the data in the relation at a particular point in time. Row/Tuple - values for each relation\u2019s attribute for one element of a relation instance ID Name Phone Dorm Age GPA 1123141 Mark 555-1234 1 19 3.21 2323411 Kim 555-9876 2 25 3.53 17642352 Sam 555-6758 1 19 3.25 Primary Key", "The Relational Database: Constraints Image borrowed from this Medium post. cannot be null Must be a valid dorm id in the dorm relation Example Constraints: 14 Constraint - conditions that must hold on all valid relation instances Types: - Key Constraints - Entity Integrity Constraints - Referential Integrity constraints ID Name Phone Dorm Age GPA 1123141 Mark 555-1234 1 19 3.21 2323411 Kim 555-9876 2 25 3.53 17642352 Sam 555-6758 1 19 3.25 Student", "The Relational Database: Relationships Name ID Phone Dorm Age GPA Mark 1123141 555-1234 1 19 3.21 Kim 2323411 555-9876 2 25 3.53 Sam 17642352 555-6758 1 19 3.25 Dorm Name 1 555 Huntington 2 Baker ID Class 1123141 COMP355 2323411 COMP355 17642352 MATH650 1123141 MATH650 2323411 BIOL110 Student Dorm Class Some values in one table are related (by design) to values in another table 15 Referential Integrity Constraint Examples", "The Relational Database: Queries Name ID Phone Dorm Age GPA Mark 1123141 555-1234 1 19 3.21 Kim 2323411 555-9876 2 25 3.53 Sam 17642352 555-6758 1 19 3.25 Dorm Name 1 555 Huntington 2 Baker ID Class 1123141 COMP355 2323411 COMP355 17642352 MATH650 1123141 MATH650 2323411 BIOL110 Student Dorm Class Questions (Queries): \u201cProvide a list of student names and IDs with GPA between 3.0 and 3.5.\u201d 16", "The Relational Database: Queries Name ID Phone Dorm Age GPA Mark 1123141 555-1234 1 19 3.21 Kim 2323411 555-9876 2 25 3.53 Sam 17642352 555-6758 1 19 3.25 Dorm Name 1 555 Huntington 2 Baker ID Class 1123141 COMP355 2323411 COMP355 17642352 MATH650 1123141 MATH650 2323411 BIOL110 Student Dorm Class 17 Questions (Queries): \u201cWhich students live in Baker Dorm?\u201d", "Databases in the Context of a Software System Database Tier Application Tier Client Tier 18", "The Relational Model of Data: Digging In 19", "History 101 Codd, Edgar F. \"A relational model of data for large shared data banks.\" Communications of the ACM 13.6 (1970): 377-387. 20 \u201cFuture users of large data banks must be protected from having to know how the data is organized in the machine (the internal representation)\u2026\u201d", "History 101 \u25cfThe relational model provides a formal mathematical basis for the structure of and interaction with a relational database \u25cb Based on set theory and \ufb01rst-order predicate logic \u25cb The formal basis allows for robust scienti\ufb01c development of the model. \u25cfThe (eternal) struggle of theory vs. practice\u2026 \u25cb Most modern RDBMSs don\u2019t strictly adhere to the purest mathematical formalisms in the relational model. 21", "A Little Set Theory Review\u2026 \u25cfWhat\u2019s a set? \u25cfWhat is the cardinality of a set? (denoted |S|) 22 A set is a collection of unique objects. The things (objects) inside a set are called elements. cardinality \u2192 the number of unique elements in the set. What are some examples of sets? ?", "A Little Set Theory Review\u2026 \u25cfWhat is the Union of sets A and B (A \u22c3 B)? \u25cfWhat is the intersection of sets A and B (A \u2229 B)? \u25cfWhat is the set di\ufb00erence of A and B (A - B)? 23 A = {1, 3, 5} B = {3, 4, 5, 6} AUB = {1, 3, 5, 4, 6} A\u22c2B = {3, 5} A-B = {1}", "A Little Set Theory Review\u2026 \u25cfWhat is the cartesian product of two sets? A = {123, 435} B = {Chul, Kev, Sal} \u25cfWhat is the cartesian product of A and B (aka A x B)? 24 AXB = { (123, Chul), (123, Kev), (123, Sal), (435, Chul), (435, Kev), (435, Sal) } The cartesian product of two sets A and B is the set of all ordered pairs (a, b) where a \u2208 A and b \u2208 B.", "What\u2019s a Relational Database? \u25cfA Relational Database consists of \u25cb a collection of relations, and \u25cb a collection of constraints. \u25cfA relational database is in a valid/consistent state if it satis\ufb01es all constraints (else, invalid/inconsistent state). 25", "Relations (Some Review) \u25cfA relation consists of \u25cb its schema \u2192 a description of the structure of the relation (relation schema) \u25cb its state \u2192 the current data that is populated in the relation (relation instance) \u25cfThe schema of a relation includes \u25cb the name of the relation \u25cb an list of n attributes each with an associated domain (what values that attribute can take on). \u25cfNotation: REL_NAME(Attrib1:Dom1, Attrib2:Dom2\u2026) 26", "More formally\u2026 \u25cfLet A1, A2, \u2026, An be names of attributes of relation R with associated domains D1, D2, \u2026, Dn, then R(A1:D1, A2:D2,...An:Dn) is a relation schema and n, the degree of R, represents the number of attributes of R. \u25cfThen, an instance of Relation R is a subset of the cartesian product of the domains of the attributes of R. 27", "Relations - Example - Assume we have the following domains: - names \u2192 {\u2018Jared\u2019, \u2018Sakshi\u2019} - id_nums \u2192 {all 9 digit positive integers starting with 00} - majors \u2192 {\u2018CS\u2019, \u2018DS\u2019, \u2018CY\u2019} - De\ufb01ning the TA relation schema: TA(name: names, id: id_nums, major: majors) 28 Is the following a valid instance of TA? { (\u2018Jared\u2019, 001928374, \u2018CS\u2019) (\u2018Sakshi\u2019, 001122334, \u2018DS\u2019) } ?", "Relations - Example - Assume we have the following domains: - names \u2192 {\u2018Jared\u2019, \u2018Sakshi\u2019} - id_nums \u2192 {all 9 digit positive integers starting with 00} - majors \u2192 {\u2018CS\u2019, \u2018DS\u2019, \u2018CY\u2019} - De\ufb01ning the TA relation schema: TA(name: names, id: id_nums, major: majors) 29 Is the following a valid instance of TA? { (\u2018Sakshi\u2019, 001928374, \u2018CS\u2019) (\u2018Sakshi\u2019, 001122334, \u2018CY\u2019) } ?", "Relations - Example - Assume we have the following domains: - names \u2192 {\u2018Jared\u2019, \u2018Sakshi\u2019} - id_nums \u2192 {all 9 digit positive integers starting with 00} - majors \u2192 {\u2018CS\u2019, \u2018DS\u2019, \u2018CY\u2019} - De\ufb01ning the TA relation schema: TA(name: names, id: id_nums, major: majors) 30 Is the following a valid instance of TA? { (\u2018Sakshi\u2019, 001928374, \u2018CS\u2019) (\u2018Dylan\u2019, 001122334, \u2018DS\u2019) } ?", "Relations - Example - Assume we have the following domains: - names \u2192 {\u2018Jared\u2019, \u2018Sakshi\u2019} - id_nums \u2192 {all 9 digit positive integers starting with 00} - majors \u2192 {\u2018CS\u2019, \u2018DS\u2019, \u2018CY\u2019} - De\ufb01ning the TA relation schema: TA(name: names, id: id_nums, major: majors) 31 Is the following a valid instance of TA? { (\u2018Sakshi\u2019, 001928374, \u2018CS\u2019) (\u2018Jared\u2019, 001122334) } ?", "Relation Instance \u25cfA relation instance is a set of tuples (rows) from a relation at a particular point in time. \u25cfEach tuple (row) is an ordered sequence of values, one for each attribute (possibly null) \u25cb usually enclosed in < and > 32 Name ID Phone Dorm Age GPA Mark 1123141 555-1234 1 19 3.21 Kim 2323411 555-9876 2 25 3.53 Sam 17642352 555-6758 1 19 3.25 Student 1 Tuple", "Null Value \u25cf Null is a special value that may exist in the domain of an attribute \u25cf Could mean di\ufb00erent things \u25cb value unknown \u25cb value unavailable right now \u25cb attribute doesn\u2019t apply to this tuple \u25cf Does NOT mean: \u25cb zero (0) \u25cb the empty string (\u2018\u2019) \u25cf (NULL != NULL) Comparing two values of NULL does NOT return true 33", "Value of an Attr in a Tuple \u25cfValues should be atomic \u25cb Say NO to composite attributes (ex: address that includes city, state and zip) \u25cb Say NO to multi-valued attributes (ex: all email addresses for 1 person) 34 Name ID Address Phone Dorm Age GPA Mark 1123141 121 Anystreet Boston MA 02212 555-1234 555-1876 1 19 3.21 Kim 2323411 235 Huntington Boston MA 02215 555-9876 2 25 3.53 Student", "Super and Candidate Keys - key - a subset of attributes of a relation used to uniquely identify each tuple - A super key of a relation R is a subset of the attributes of R such that no two distinct tuples in any possible relation instance will have the same values for the subset of attributes. - may not be minimal - could contain attributes that aren\u2019t needed for unique determination - A candidate key of relation r is a minimal super key. - A relation may have more than one candidate keys. 35", "Keys - Superkey Some Possible Superkeys: (customerNumber, customerName) (customerNumber, salesRepEmployeeNumber) (customerNumber) Not Minimal Minimal Customers 36", "Keys - Candidate Keys Some Possible Superkeys: (customerNumber, customerName) (customerNumber, salesRepEmployeeNumber) (customerNumber) Not Candidate Keys Candidate Key Customers 37", "The Primary Key - The primary key (PK) of relation R is chosen from the set of candidate keys - If a relation has only 1 candidate key, it becomes the PK. - If a relation has > 1 candidate key, the database designer chooses one based on business requirements - Every relation must have a PK - Entity Integrity Constraint \u2192 PK values must be unique and may NOT be null - (Usually) the PK is underlined in a relation schema or table 38", "Keys - Primary Key Some Possible Superkeys: (customerNumber, customerName) (customerNumber, salesRepEmployeeNumber) (customerNumber) Chosen as Primary Key If there are 2+ Candidate Keys, DB Designer will choose one as the Primary key Customers 39", "Foreign Keys \u25cf Foreign Key (FK) - An attribute ai in one relation RC (the child relation) refers to/references the PK aj in another relation RP (parent relation) such that all values of ai must either be NULL or contain a value from aj. \u25cf Self-Referential Relation \u2192 RC and RP are the same relation \u25cf Foreign Key == Referential Integrity Constraint \u25cf Foreign Keys are the operationalization of relationships in a relational database 40", "Foreign Key Example 41 Students cID sID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades sID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Parent Relation Child Relation (the one being referenced) (the one referencing another relation) Primary Keys Foreign Key Note: in this example, Grades.sID cannot contain null values because it is part of the PK of Grades Relation.", "FKs - but why??? Customers Notice: empLast, empFirst, and empEmail attributes contain duplicated/repeated data. Opens up the possibility of insert or update anomalies. So, put them in a separate table along with empNum and refer back to the new table (becomes the parent table) 42", "Relational Algebra 43", "Relational Algebra \u25cfRelational Algebra (RA) : a procedural query language for relations that allow us to retrieve information from a relational database \u25cb A query is an operation or set of operations applied to one or more relation instances \u25cb RA is closed, meaning the result/output of each query is another relation \u25cb In RA, order of operations matters \u25cfNot a full-\ufb02edged (turing complete) programming language 44", "Quick Aside: Predicate Functions - Predicate functions - functions that return true or false. - We will use predicate functions (or just \u201cpredicates\u201d) to determine if tuples in a relation instance should be returned by a query or not. - in the form attr <op> attr OR attr <op> <constant> - <op> can be any standard relational operator (=, !=, <, >, <=, etc) - predicates can be composed with ^ (and), v (or), \u3131 (not). empID firstName 333 Bob 143 Sam Employees Examples: - empID = 143 - empID > 400 - firstName = \u2018Bob\u2019 ^ empID = 333 45", "The First 8 Basic Operations of RA 1. Select \u03c3 (Tuple Filtering) 2. Project \u03c0 (Attribute Filtering) 3. Rename \u03c1 4. Cartesian Product \u2715 5. Join \u2a1d 6. Intersection \u22c2 7. Set-di\ufb00erence \u2013 8. Union U 46 Highest Precedence Lowest Precedence You can always use ( ) to change the order of operations. Often times, ( ) make things more clear.", "Relational Algebra: Select Operator \u25cfSelect - return a relation containing tuples from relation R that satisfy predicate pred. Notation: \u25cfThink of it as a horizontal subset (subset of tuples/rows) of a relation instance 47", "Relational Algebra: Select Operator 48 Dept Class Enrollment CS 3200 40 CS 2500 643 CS 1800 680 DS 2000 412 Enrollments Dept Class Enrollment CS 2500 643 CS 1800 680 Enrollments Result:", "Relational Algebra: Select Operator 49 Dept Class Taught_by CS 3200 CS CS 2500 CS CS 1800 Math DS 2000 Math Enrollments Dept Class Taught_by CS 3200 CS Enrollments Result:", "Relational Algebra: Project Operator Project - returns a relation with a subset of attributes (A1\u2026 Ak) from R. Notation: Duplicate tuples will be removed from the resulting relation (because relations are sets). 50", "Relational Algebra: Project Operator 51 Dept Class Enrollment CS 3200 40 CS 2500 643 CS 1800 680 DS 2000 412 Enrollments Dept Class CS 3200 CS 2500 CS 1800 DS 2000 Enrollments Dept CS DS Enrollments Result: Result:", "Relational Algebra: Cartesian Product Same operation from set theory. 52 Attr_1 Attr_2 123 abc 456 def R Attr_1 Attr_3 123 CS 789 DS 111 Cyber S R.Attr_1 R.Attr_2 S.Attr_1 S.Attr_3 123 abc 123 CS 123 abc 789 DS 123 abc 111 Cyber 456 def 123 CS 456 def 789 DS 456 def 111 Cyber Note: we can always use Relation.Attribute notation to resolve naming collisions. Relations can\u2019t have two attributes with the same name", "Union, Intersection & Difference \u25cfEssentially the same as what we know from set theory. \u25cfOne small di\ufb00erence: relations must be schema compatible \u25cb same number of attributes \u25cb attributes\u2019 domains must be compatible 53", "More Complex RA Expressions 54 - Simple RA expression can be composed into more complex expressions - Remember: output of each RA operation is another relation", "Relational Algebra: Temporary Relation Names For more complex RA queries, you can have: \u25cf one long query expression \u25cf an ordered list of smaller expressions, the result of each is given a temporary name with the \u2190 operator Example: \u25cf TEMP_NAME \u2190 TEMP_NAME can then be used as a relation in subsequent steps of the same query. \u25cf Be careful about attribute naming collisions 55", "Relational Algebra: Rename Operator Rename Operator (rho) \u2013 Allows us to \u201crename\u201d a relation, the attributes of a relation, or both. - If only name is provided, the relation is being renamed (all attributes retain their original name.) - List of attributes in parentheses means renaming attributes, but not relation (assume attributes originally (employeeID, lastName, \ufb01rstName) - Rename both. 56", "Developing Relational Algebra Expressions 57", "Writing RA Queries - Sometimes we need to evaluate an RA query against a database instance - Result is usually another relation instance/set of tuples/table - Other times we need to convert the narrative form of a query into a RA query - Example: \u201cProvide a list of all info from the Employee relation where the empID is less than 400.\u201d - Answer: 58", "Writing RA Queries 59 Stu_ID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Cou_ID Dept Number Prof 332 CS 3345 Lawrimore 221 CS 2341 Fontenot 535 MATH 2339 Norris Students Courses C_ID S_ID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades Next few examples use this database schema.", "Writing RA Queries 60 Stu_ID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Cou_ID Dept Number Prof 332 CS 3345 Lawrimore 221 CS 2341 Fontenot 535 MATH 2339 Norris Students Courses C_ID S_ID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades Write a RA query that returns the names of all students.", "Writing RA Queries 61 Stu_ID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Cou_ID Dept Number Prof 332 CS 3345 Lawrimore 221 CS 2341 Fontenot 535 MATH 2339 Norris Students Courses C_ID S_ID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades Provide a list of all course numbers taught by the CS Department.", "Writing RA Queries 62 Stu_ID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Cou_ID Dept Number Prof 332 CS 3345 Lawrimore 221 CS 2341 Fontenot 535 MATH 2339 Norris Students Courses C_ID S_ID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades List all 2nd year student names.", "Writing RA Queries 63 Stu_ID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Cou_ID Dept Number Prof 332 CS 3345 Lawrimore 221 CS 2341 Fontenot 535 MATH 2339 Norris Students Courses C_ID S_ID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades What course or courses (dept and number) are taught by Professor Norris?", "Writing RA Queries 64 Stu_ID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Cou_ID Dept Number Prof 332 CS 3345 Lawrimore 221 CS 2341 Fontenot 535 MATH 2339 Norris Students Courses C_ID S_ID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades List the letter grades that Sam has earned (don\u2019t need to include course info). or", "Joining Data from Two Relations 65", "Relational Algebra: The Join Operator \u25cfJoin - allows us to combine data from two relations. \u25cb Subset of the cartesian product of the two argument relations based on explicit or implicit join predicate. \u25cf2 Versions: \u25cb Natural Join - Join relations on attributes with the same name \u25cb Theta Join (or condition join or simply Join) - Join relations with explicitly supplied join predicate 66", "Natural Join \u25cfGiven: A(R, S, T, U, V) and B(S, V, W, X) \u25cfQuery: \u25cfNotice: A and B both have attributes named S & V. \u25cfResult: \u25cb Schema of resulting relation is where attr(R) returns a set containing the attributes of R - so, attributes used in the implicit join condition are not duplicated in the result \u25cb contains any tuple from A x B where values for attributes S and V are equal \u25cb If A & B have no common attribute names, result is A X B. 67", "Example 68 Students cID sID Grade 332 234 B 332 332 A 535 336 C 221 134 A 535 332 C 221 332 A 221 336 B Grades sID Name Year 134 Chul 1 234 Kev 3 332 Sam 2 336 Ashwin 2 Query: cID sID Grade Name Year 332 234 B Kev 3 332 332 A Sam 2 535 336 C Ashwin 2 221 134 A Chul 1 535 332 C Sam 2 221 332 A Sam 2 221 336 B Ashwin 2 Result:", "theta-Join (or Join\u2026 or Condition(al) Join) \u25cf Operator has an explicit join predicate (condition) (doesn\u2019t rely upon attribute names) \u25cf Result is a subset of the cartesian product where provided predicate holds true \u25cf Assume: S(sID, name) and G(stu-ID, course, semester, grade) \u25cf Query: \u25cf Result: \u25cb Relation with schema (sID, name, stu-ID, course, semester, grade) \u25cb all tuples where S.sID = G.stu-ID \u25cf Note: join condition can use other operators besides =. 69", "theta-Join (or Join\u2026 or Condition(al) Join) A B 1 Cat 2 Dog 3 Bird C D 1 Meow 3 Chirp S T A B 1 Cat 3 Bird C D 1 Meow 3 Chirp 70", "New: Entity Relationship Diagram 71 Northwind Traders Data Model - sample database model that has been around for years; originally developed by Microsoft - represents data model for fake Northwind Traders company, an importer/exporter of specialty foods", "New: ER Diagram 72 Relation Name Attributes Primary Key Attributes Relationships", "Relations in Northwind \u25cfSuppliers: Suppliers and vendors of Northwind \u25cfCustomers: Customers who buy products from Northwind \u25cfEmployees: Employee details of Northwind traders \u25cfProducts: Product information \u25cfShippers: The details of the shippers who ship the products from the traders to the end-customers \u25cfOrders and Order_Details: Sales Order transactions taking place between the customers & the company \u25cfCategories: The categories a product can fall into 73", "For each of the following, compose a relational algebra query that satis\ufb01es the query prompt. Practice Time! 74", "Query Time! 75 1. For each supplier, provide its name, contact person, and phone number.", "Query Time! 76 2. Which products are supplied by FoodsRUs? Please include the product name and unit price.", "Query Time! 77 3. Provide a list of all product names ordered by World Market (a customer\u2019s name).", "Query Time! 78 4. Which customers has employee Sam Johnson worked with? Provide the complete customer information. (CustomerID)", "Further Reading \u25cfHarrington Ch5 (OReilly) \u25cfFoundations of Computer Science - Ch 8 - The Relational Model 79"]